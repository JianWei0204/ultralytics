# Ultralytics YOLO ğŸš€, AGPL-3.0 license

"""
å¤šå°ºåº¦ç‰¹å¾æå–å™¨ (Multi-scale Feature Extractor)
ç”¨äºDayâ†’RainåŸŸé€‚åº”çš„å¤šå°ºåº¦ç‰¹å¾æå–å’Œèåˆ

ä¸»è¦åŠŸèƒ½ï¼š
1. ä»YOLOv8éª¨å¹²ç½‘ç»œæå–å¤šå°ºåº¦ç‰¹å¾
2. è·¨å°ºåº¦ç‰¹å¾èåˆ
3. åŸŸä¸å˜ç‰¹å¾å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional


class MultiScaleFeatureExtractor(nn.Module):
    """
    å¤šå°ºåº¦ç‰¹å¾æå–å™¨
    
    ä»YOLOv8çš„ä¸åŒå±‚æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œç”¨äºåŸŸé€‚åº”è®­ç»ƒ
    """
    
    def __init__(self, 
                 feature_dims: List[int] = [256, 512, 1024],
                 extract_layers: List[str] = ['backbone.9', 'backbone.12', 'backbone.15']):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦ç‰¹å¾æå–å™¨
        
        Args:
            feature_dims: å„å°ºåº¦ç‰¹å¾çš„ç»´åº¦åˆ—è¡¨
            extract_layers: è¦æå–ç‰¹å¾çš„å±‚åç§°åˆ—è¡¨
        """
        super(MultiScaleFeatureExtractor, self).__init__()
        
        self.feature_dims = feature_dims
        self.extract_layers = extract_layers
        self.extracted_features = {}
        self.hooks = []
        
        # ç‰¹å¾é€‚é…å±‚ï¼Œå°†ä¸åŒç»´åº¦çš„ç‰¹å¾è½¬æ¢ä¸ºç»Ÿä¸€ç»´åº¦
        self.feature_adapters = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(dim, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ) for dim in feature_dims
        ])
        
        # å¤šå°ºåº¦èåˆæ¨¡å—
        self.fusion_module = FusionModule(256, len(feature_dims))
        
    def register_hooks(self, model):
        """
        ä¸ºæŒ‡å®šçš„å±‚æ³¨å†Œå‰å‘é’©å­å‡½æ•°
        
        Args:
            model: YOLOv8æ¨¡å‹
        """
        self.clear_hooks()
        
        for name, module in model.named_modules():
            if name in self.extract_layers:
                hook = module.register_forward_hook(self._hook_fn(name))
                self.hooks.append(hook)
                
    def _hook_fn(self, layer_name):
        """
        åˆ›å»ºé’©å­å‡½æ•°
        
        Args:
            layer_name: å±‚åç§°
        """
        def hook(module, input, output):
            self.extracted_features[layer_name] = output
        return hook
        
    def clear_hooks(self):
        """æ¸…é™¤æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        self.extracted_features = {}
        
    def forward(self, x=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ (å¯é€‰ï¼Œä¸»è¦é€šè¿‡é’©å­è·å–ç‰¹å¾)
            
        Returns:
            Dict: åŒ…å«åŸå§‹ç‰¹å¾ã€é€‚é…ç‰¹å¾å’Œèåˆç‰¹å¾çš„å­—å…¸
        """
        if not self.extracted_features:
            return {}
            
        # è·å–æå–çš„ç‰¹å¾
        raw_features = []
        for layer_name in self.extract_layers:
            if layer_name in self.extracted_features:
                raw_features.append(self.extracted_features[layer_name])
                
        if not raw_features:
            return {}
            
        # ç‰¹å¾é€‚é…
        adapted_features = []
        for i, feature in enumerate(raw_features):
            if i < len(self.feature_adapters):
                adapted = self.feature_adapters[i](feature)
                adapted_features.append(adapted)
                
        # å¤šå°ºåº¦èåˆ
        if adapted_features:
            fused_features = self.fusion_module(adapted_features)
        else:
            fused_features = None
            
        return {
            'raw_features': raw_features,
            'adapted_features': adapted_features,
            'fused_features': fused_features
        }


class FusionModule(nn.Module):
    """
    ç‰¹å¾èåˆæ¨¡å—
    
    å°†å¤šä¸ªå°ºåº¦çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œç”ŸæˆåŸŸä¸å˜çš„è¡¨ç¤º
    """
    
    def __init__(self, feature_dim: int = 256, num_scales: int = 3):
        """
        åˆå§‹åŒ–èåˆæ¨¡å—
        
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦
            num_scales: å°ºåº¦æ•°é‡
        """
        super(FusionModule, self).__init__()
        
        self.feature_dim = feature_dim
        self.num_scales = num_scales
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        self.attention_conv = nn.Conv2d(feature_dim * num_scales, num_scales, 1)
        
        # ç‰¹å¾èåˆå·ç§¯
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(feature_dim * num_scales, feature_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, feature_dim, 1, bias=False),
            nn.BatchNorm2d(feature_dim)
        )
        
        # æ®‹å·®è¿æ¥
        self.residual_conv = nn.Conv2d(feature_dim, feature_dim, 1, bias=False)
        
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features: å¤šå°ºåº¦ç‰¹å¾åˆ—è¡¨
            
        Returns:
            torch.Tensor: èåˆåçš„ç‰¹å¾
        """
        if not features:
            return None
            
        # å°†æ‰€æœ‰ç‰¹å¾è°ƒæ•´åˆ°ç›¸åŒçš„ç©ºé—´å°ºå¯¸
        target_size = features[0].shape[2:]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾çš„å°ºå¯¸ä½œä¸ºç›®æ ‡
        
        resized_features = []
        for feature in features:
            if feature.shape[2:] != target_size:
                # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è°ƒæ•´å°ºå¯¸
                resized = F.interpolate(feature, size=target_size, mode='bilinear', align_corners=False)
            else:
                resized = feature
            resized_features.append(resized)
            
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        concatenated = torch.cat(resized_features, dim=1)  # [B, C*N, H, W]
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(self.attention_conv(concatenated), dim=1)  # [B, N, H, W]
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        weighted_features = []
        for i, feature in enumerate(resized_features):
            weight = attention_weights[:, i:i+1, :, :]  # [B, 1, H, W]
            weighted = feature * weight
            weighted_features.append(weighted)
            
        # åŠ æƒæ±‚å’Œ
        weighted_sum = torch.stack(weighted_features, dim=0).sum(dim=0)  # [B, C, H, W]
        
        # ç‰¹å¾èåˆ
        fused = self.fusion_conv(concatenated)
        
        # æ®‹å·®è¿æ¥
        residual = self.residual_conv(weighted_sum)
        output = fused + residual
        
        return F.relu(output, inplace=True)


class DomainInvariantHead(nn.Module):
    """
    åŸŸä¸å˜ç‰¹å¾å­¦ä¹ å¤´
    
    ç”¨äºå­¦ä¹ åŸŸä¸å˜çš„ç‰¹å¾è¡¨ç¤º
    """
    
    def __init__(self, input_dim: int = 256, hidden_dim: int = 128):
        """
        åˆå§‹åŒ–åŸŸä¸å˜ç‰¹å¾å­¦ä¹ å¤´
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(DomainInvariantHead, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, C, H, W]
            
        Returns:
            torch.Tensor: åŸŸä¸å˜ç‰¹å¾ [B, hidden_dim//2]
        """
        return self.feature_extractor(x)