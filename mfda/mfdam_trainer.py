# Ultralytics YOLO ğŸš€, AGPL-3.0 license

"""
MFDAMè®­ç»ƒå™¨ (Multi-scale Fusion Domain Adaptation Module Trainer)
ä¸“é—¨ç”¨äºDayâ†’RainåŸŸé€‚åº”çš„YOLOv8è®­ç»ƒå™¨

ä¸»è¦åŠŸèƒ½ï¼š
1. é›†æˆå¤šå°ºåº¦ç‰¹å¾æå–
2. åŸŸå¯¹æŠ—è®­ç»ƒ
3. ç‰¹å¾èåˆå’Œå¯¹é½
4. è®­ç»ƒè¿‡ç¨‹ç›‘æ§å’Œå¯è§†åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import time
import math
from typing import Dict, Optional, Tuple, List
from pathlib import Path
from tqdm import tqdm

from ultralytics.models.yolo.detect import DetectionTrainer
from ultralytics.utils import LOGGER, RANK, colorstr
from ultralytics.utils.torch_utils import de_parallel
from ultralytics.utils.loss import v8DetectionLoss

try:
    from .multi_scale_extractor import MultiScaleFeatureExtractor, DomainInvariantHead
    from .domain_losses import MFDAMLoss
    from .dual_dataloader import DualDomainDataLoader
    from .config import MFDAMConfig
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜
    from mfda.multi_scale_extractor import MultiScaleFeatureExtractor, DomainInvariantHead
    from mfda.domain_losses import MFDAMLoss
    from mfda.dual_dataloader import DualDomainDataLoader
    from mfda.config import MFDAMConfig


class MFDAMTrainer(DetectionTrainer):
    """
    MFDAMè®­ç»ƒå™¨
    
    æ‰©å±•æ ‡å‡†YOLOv8æ£€æµ‹è®­ç»ƒå™¨ï¼Œæ·»åŠ å¤šå°ºåº¦èåˆåŸŸé€‚åº”åŠŸèƒ½
    """
    
    def __init__(self, cfg=None, overrides=None, _callbacks=None):
        """
        åˆå§‹åŒ–MFDAMè®­ç»ƒå™¨
        
        Args:
            cfg: é…ç½®å¯¹è±¡
            overrides: è¦†ç›–å‚æ•°
            _callbacks: å›è°ƒå‡½æ•°
        """
        super().__init__(cfg, overrides, _callbacks)
        
        # MFDAMç‰¹å®šç»„ä»¶
        self.mfdam_config = None
        self.feature_extractor = None
        self.domain_loss_fn = None
        self.dual_dataloader = None
        self.domain_invariant_head = None
        
        # ä¼˜åŒ–å™¨
        self.optimizer_da = None  # åŸŸé€‚åº”ä¼˜åŒ–å™¨
        
        # è®­ç»ƒçŠ¶æ€
        self.da_losses = {}
        self.da_metrics = {}
        
        LOGGER.info(f"{colorstr('cyan', 'bold', 'MFDAM Trainer')} åˆå§‹åŒ–å®Œæˆ")
        
    def setup_mfdam(self, 
                    target_data: str,
                    mfdam_config: Optional[MFDAMConfig] = None):
        """
        è®¾ç½®MFDAMç»„ä»¶
        
        Args:
            target_data: ç›®æ ‡åŸŸæ•°æ®é…ç½®è·¯å¾„
            mfdam_config: MFDAMé…ç½®å¯¹è±¡
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–æä¾›çš„é…ç½®
        self.mfdam_config = mfdam_config or MFDAMConfig()
        
        # åˆå§‹åŒ–å¤šå°ºåº¦ç‰¹å¾æå–å™¨
        self.feature_extractor = MultiScaleFeatureExtractor(
            feature_dims=self.mfdam_config.feature_dims,
            extract_layers=self.mfdam_config.extract_layers
        )
        
        # æ³¨å†Œç‰¹å¾æå–é’©å­
        if self.model is not None:
            self.feature_extractor.register_hooks(self.model)
            
        # åˆå§‹åŒ–åŸŸä¸å˜ç‰¹å¾å­¦ä¹ å¤´
        self.domain_invariant_head = DomainInvariantHead(
            input_dim=self.mfdam_config.fusion_dim,
            hidden_dim=self.mfdam_config.da_hidden_dim
        )
        
        # åˆå§‹åŒ–åŸŸé€‚åº”æŸå¤±å‡½æ•°
        self.domain_loss_fn = MFDAMLoss(
            domain_weight=self.mfdam_config.domain_weight,
            consistency_weight=self.mfdam_config.consistency_weight,
            alignment_weight=self.mfdam_config.alignment_weight,
            weather_weight=self.mfdam_config.weather_weight
        )
        
        # åˆ›å»ºåŒåŸŸæ•°æ®åŠ è½½å™¨
        self.dual_dataloader = DualDomainDataLoader(
            source_cfg=self.args.data,  # ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ•°æ®ä½œä¸ºæºåŸŸ
            target_cfg=target_data,
            batch_size=self.args.batch,
            img_size=self.args.imgsz,
            workers=self.args.workers,
            domain_balance=self.mfdam_config.domain_balance
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(self, 'device'):
            self.domain_invariant_head.to(self.device)
            self.domain_loss_fn.to(self.device)
            
        LOGGER.info(f"{colorstr('green', 'MFDAMç»„ä»¶è®¾ç½®å®Œæˆ')}")
        
    def _setup_train(self, world_size):
        """
        è®¾ç½®è®­ç»ƒç¯å¢ƒ
        
        Args:
            world_size: ä¸–ç•Œå¤§å°ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»è®¾ç½®
        super()._setup_train(world_size)
        
        # è®¾ç½®åŸŸé€‚åº”ä¼˜åŒ–å™¨
        if self.domain_invariant_head is not None and self.domain_loss_fn is not None:
            da_params = list(self.domain_invariant_head.parameters()) + \
                       list(self.domain_loss_fn.parameters())
            
            self.optimizer_da = torch.optim.Adam(
                da_params,
                lr=self.mfdam_config.da_lr,
                weight_decay=self.mfdam_config.da_weight_decay
            )
            
        LOGGER.info(f"{colorstr('green', 'MFDAMè®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ')}")
        
    def _do_train(self, world_size=1):
        """
        æ‰§è¡ŒMFDAMè®­ç»ƒ
        
        Args:
            world_size: ä¸–ç•Œå¤§å°
        """
        if self.dual_dataloader is None:
            LOGGER.warning("åŒåŸŸæ•°æ®åŠ è½½å™¨æœªè®¾ç½®ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒ")
            return super()._do_train(world_size)
            
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.epochs):
            self.epoch = epoch
            
            # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
            self.model.train()
            if self.domain_invariant_head is not None:
                self.domain_invariant_head.train()
                
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.da_losses = {}
            self.da_metrics = {}
            
            # è¿›åº¦æ¡
            pbar = enumerate(self.dual_dataloader)
            if RANK in (-1, 0):
                pbar = tqdm(pbar, total=len(self.dual_dataloader), desc=f"Epoch {epoch+1}/{self.epochs}")
                
            for batch_idx, batch in pbar:
                # æ‰§è¡Œä¸€æ­¥è®­ç»ƒ
                self._train_step(batch, batch_idx)
                
                # æ›´æ–°è¿›åº¦æ¡
                if RANK in (-1, 0) and batch_idx % 10 == 0:
                    pbar.set_postfix(self._get_progress_info())
                    
            # éªŒè¯å’Œä¿å­˜
            self._end_epoch()
            
    def _train_step(self, batch: Dict, batch_idx: int):
        """
        æ‰§è¡Œä¸€æ­¥è®­ç»ƒ
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
        """
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        imgs = batch['img'].to(self.device, non_blocking=True)
        domain_labels = batch['domain_labels'].to(self.device, non_blocking=True)
        weather_labels = batch['weather_labels'].to(self.device, non_blocking=True)
        
        # åˆ†ç¦»æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®
        source_mask = domain_labels == 0
        target_mask = domain_labels == 1
        
        source_imgs = imgs[source_mask]
        target_imgs = imgs[target_mask]
        
        # å‰å‘ä¼ æ’­
        with torch.cuda.amp.autocast(enabled=self.amp):
            # æ ‡å‡†æ£€æµ‹æŸå¤±ï¼ˆä»…ç”¨æºåŸŸæ•°æ®ï¼‰
            if source_imgs.size(0) > 0:
                # å‡†å¤‡æºåŸŸæ•°æ®ç”¨äºæ£€æµ‹è®­ç»ƒ
                source_batch = self._prepare_detection_batch(batch, source_mask)
                preds = self.model(source_imgs)
                self.loss, self.loss_items = self.criterion(preds, source_batch)
            else:
                self.loss = torch.tensor(0.0, device=self.device)
                self.loss_items = torch.zeros(3, device=self.device)
            
            # å¤šå°ºåº¦ç‰¹å¾æå–
            _ = self.model(imgs)  # è§¦å‘ç‰¹å¾æå–é’©å­
            feature_results = self.feature_extractor()
            
            # åŸŸé€‚åº”æŸå¤±
            da_loss = torch.tensor(0.0, device=self.device)
            if feature_results and 'fused_features' in feature_results:
                fused_features = feature_results['fused_features']
                
                if fused_features is not None and source_imgs.size(0) > 0 and target_imgs.size(0) > 0:
                    source_features = fused_features[source_mask]
                    target_features = fused_features[target_mask]
                    
                    # è®¡ç®—åŸŸé€‚åº”æŸå¤±
                    da_results = self.domain_loss_fn(
                        source_features=source_features,
                        target_features=target_features,
                        weather_labels=weather_labels
                    )
                    
                    da_loss = da_results['total_da_loss']
                    self.da_losses.update(da_results)
                    
            # æ€»æŸå¤±
            total_loss = self.loss + self.mfdam_config.da_loss_weight * da_loss
            
        # åå‘ä¼ æ’­
        self.scaler.scale(total_loss).backward()
        
        # æ›´æ–°å‚æ•°
        if batch_idx % self.accumulate == 0:
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
            
            # æ›´æ–°åŸŸé€‚åº”å‚æ•°
            if self.optimizer_da is not None and da_loss > 0:
                self.optimizer_da.step()
                self.optimizer_da.zero_grad()
                
    def _prepare_detection_batch(self, batch: Dict, mask: torch.Tensor) -> Dict:
        """
        å‡†å¤‡æ£€æµ‹è®­ç»ƒæ‰¹æ¬¡
        
        Args:
            batch: åŸå§‹æ‰¹æ¬¡
            mask: æºåŸŸæ©ç 
            
        Returns:
            Dict: æ£€æµ‹è®­ç»ƒæ‰¹æ¬¡
        """
        detection_batch = {}
        
        # å¤åˆ¶éœ€è¦çš„å­—æ®µ
        for key in ['bboxes', 'cls', 'batch_idx']:
            if key in batch:
                data = batch[key]
                if hasattr(data, 'shape') and len(data.shape) > 0:
                    # è¿‡æ»¤æºåŸŸæ•°æ®
                    if key == 'batch_idx':
                        # é‡æ–°æ˜ å°„æ‰¹æ¬¡ç´¢å¼•
                        source_indices = torch.where(mask)[0]
                        batch_mask = torch.isin(data, source_indices)
                        filtered_data = data[batch_mask]
                        # é‡æ–°ç¼–å·
                        for i, old_idx in enumerate(source_indices):
                            filtered_data[filtered_data == old_idx] = i
                        detection_batch[key] = filtered_data
                    else:
                        # å…¶ä»–æ•°æ®ç›´æ¥è¿‡æ»¤
                        if data.size(0) > 0:
                            batch_indices = data[:, 0].long()
                            source_indices = torch.where(mask)[0]
                            batch_mask = torch.isin(batch_indices, source_indices)
                            detection_batch[key] = data[batch_mask]
                        else:
                            detection_batch[key] = data
                else:
                    detection_batch[key] = data
                    
        return detection_batch
        
    def _get_progress_info(self) -> Dict:
        """
        è·å–è®­ç»ƒè¿›åº¦ä¿¡æ¯
        
        Returns:
            Dict: è¿›åº¦ä¿¡æ¯
        """
        info = {}
        
        # æ£€æµ‹æŸå¤±
        if hasattr(self, 'loss_items'):
            info['det_loss'] = f"{self.loss_items.mean():.4f}"
            
        # åŸŸé€‚åº”æŸå¤±
        if 'total_da_loss' in self.da_losses:
            info['da_loss'] = f"{self.da_losses['total_da_loss']:.4f}"
            
        if 'domain_accuracy' in self.da_losses:
            info['dom_acc'] = f"{self.da_losses['domain_accuracy']:.3f}"
            
        return info
        
    def _end_epoch(self):
        """ç»“æŸä¸€ä¸ªepochçš„å¤„ç†"""
        # å­¦ä¹ ç‡è°ƒåº¦
        if self.scheduler is not None:
            self.scheduler.step()
            
        if self.optimizer_da is not None and hasattr(self, 'da_scheduler'):
            self.da_scheduler.step()
            
        # éªŒè¯
        if self.epoch % self.args.val_period == 0:
            self.validate()
            
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if self.epoch % self.args.save_period == 0:
            self.save_model()
            
    def validate(self):
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        # è°ƒç”¨çˆ¶ç±»éªŒè¯
        results = super().validate()
        
        # æ·»åŠ åŸŸé€‚åº”ç›¸å…³çš„éªŒè¯æŒ‡æ ‡
        if self.dual_dataloader is not None:
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åŸŸé€‚åº”ç‰¹å®šçš„éªŒè¯é€»è¾‘
            pass
            
        return results
        
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        # è°ƒç”¨çˆ¶ç±»ä¿å­˜
        super().save_model()
        
        # ä¿å­˜MFDAMç‰¹å®šç»„ä»¶
        if self.domain_invariant_head is not None:
            save_path = self.save_dir / f'mfdam_components_epoch_{self.epoch}.pt'
            torch.save({
                'domain_head': self.domain_invariant_head.state_dict(),
                'domain_loss': self.domain_loss_fn.state_dict() if self.domain_loss_fn else None,
                'optimizer_da': self.optimizer_da.state_dict() if self.optimizer_da else None,
                'config': self.mfdam_config.__dict__ if self.mfdam_config else None
            }, save_path)
            
    def load_mfdam_components(self, checkpoint_path: str):
        """
        åŠ è½½MFDAMç»„ä»¶
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        """
        if not os.path.exists(checkpoint_path):
            LOGGER.warning(f"MFDAMæ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return
            
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        if self.domain_invariant_head is not None and 'domain_head' in checkpoint:
            self.domain_invariant_head.load_state_dict(checkpoint['domain_head'])
            
        if self.domain_loss_fn is not None and 'domain_loss' in checkpoint:
            self.domain_loss_fn.load_state_dict(checkpoint['domain_loss'])
            
        if self.optimizer_da is not None and 'optimizer_da' in checkpoint:
            self.optimizer_da.load_state_dict(checkpoint['optimizer_da'])
            
        LOGGER.info(f"MFDAMç»„ä»¶åŠ è½½å®Œæˆ: {checkpoint_path}")
        
    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ¸…ç†ç‰¹å¾æå–é’©å­
        if hasattr(self, 'feature_extractor') and self.feature_extractor is not None:
            self.feature_extractor.clear_hooks()


def train_mfdam(model_cfg: str,
                source_data: str,
                target_data: str,
                epochs: int = 100,
                batch_size: int = 16,
                imgsz: int = 640,
                **kwargs):
    """
    MFDAMè®­ç»ƒä¾¿æ·å‡½æ•°
    
    Args:
        model_cfg: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
        source_data: æºåŸŸæ•°æ®é…ç½®è·¯å¾„
        target_data: ç›®æ ‡åŸŸæ•°æ®é…ç½®è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        imgsz: å›¾åƒå°ºå¯¸
        **kwargs: å…¶ä»–å‚æ•°
    """
    from ultralytics.cfg import get_cfg
    from ultralytics.utils import DEFAULT_CFG
    
    # åˆ›å»ºè®­ç»ƒå‚æ•°
    train_args = {
        'model': model_cfg,
        'data': source_data,
        'epochs': epochs,
        'batch': batch_size,
        'imgsz': imgsz,
        **kwargs
    }
    
    # åˆ›å»ºé…ç½®
    cfg = get_cfg(DEFAULT_CFG, {})
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MFDAMTrainer(cfg=cfg, overrides=train_args)
    
    # è®¾ç½®MFDAM
    trainer.setup_mfdam(target_data=target_data)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    return trainer