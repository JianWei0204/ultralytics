# Ultralytics YOLO ğŸš€, AGPL-3.0 license

"""
æƒé‡åŠ è½½å™¨ (Weight Loader)
ç”¨äºåŠ è½½å’Œç®¡ç†YOLOv8é¢„è®­ç»ƒæƒé‡

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒYOLOv8æƒé‡
2. æƒé‡å…¼å®¹æ€§æ£€æŸ¥
3. å±‚çº§æƒé‡æ˜ å°„
4. åŸŸé€‚åº”æƒé‡åˆå§‹åŒ–
"""

import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, Optional, Union
import logging
import time

from ultralytics.utils import LOGGER


def load_pretrained_weights(model: nn.Module, 
                          weights_path: Union[str, Path],
                          strict: bool = False,
                          exclude_keys: Optional[list] = None) -> Dict:
    """
    åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°æ¨¡å‹
    
    Args:
        model: ç›®æ ‡æ¨¡å‹
        weights_path: æƒé‡æ–‡ä»¶è·¯å¾„
        strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…é”®å
        exclude_keys: è¦æ’é™¤çš„é”®ååˆ—è¡¨
        
    Returns:
        Dict: åŠ è½½ç»“æœä¿¡æ¯
    """
    weights_path = Path(weights_path)
    
    if not weights_path.exists():
        LOGGER.warning(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
        return {"status": "failed", "reason": "file_not_found"}
    
    try:
        # åŠ è½½æƒé‡æ–‡ä»¶
        checkpoint = torch.load(weights_path, map_location='cpu')
        
        # æå–æ¨¡å‹çŠ¶æ€å­—å…¸
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint.state_dict()
            
        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
        model_dict = model.state_dict()
        
        # è¿‡æ»¤ä¸å…¼å®¹çš„é”®
        exclude_keys = exclude_keys or []
        filtered_dict = {}
        
        for k, v in state_dict.items():
            # è·³è¿‡æ’é™¤çš„é”®
            if any(exclude_key in k for exclude_key in exclude_keys):
                continue
                
            # æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨äºç›®æ ‡æ¨¡å‹ä¸­
            if k in model_dict:
                # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
                if v.shape == model_dict[k].shape:
                    filtered_dict[k] = v
                else:
                    LOGGER.warning(f"å½¢çŠ¶ä¸åŒ¹é…: {k} {v.shape} vs {model_dict[k].shape}")
            else:
                LOGGER.debug(f"é”®ä¸å­˜åœ¨äºç›®æ ‡æ¨¡å‹: {k}")
                
        # æ›´æ–°æ¨¡å‹å­—å…¸
        model_dict.update(filtered_dict)
        
        # åŠ è½½æƒé‡
        missing_keys, unexpected_keys = model.load_state_dict(model_dict, strict=strict)
        
        # ç»Ÿè®¡ä¿¡æ¯
        loaded_keys = len(filtered_dict)
        total_keys = len(state_dict)
        
        LOGGER.info(f"æƒé‡åŠ è½½å®Œæˆ: {loaded_keys}/{total_keys} å±‚")
        
        if missing_keys:
            LOGGER.warning(f"ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
            for key in missing_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                LOGGER.debug(f"  - {key}")
                
        if unexpected_keys:
            LOGGER.warning(f"æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
            for key in unexpected_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                LOGGER.debug(f"  - {key}")
                
        return {
            "status": "success",
            "loaded_keys": loaded_keys,
            "total_keys": total_keys,
            "missing_keys": missing_keys,
            "unexpected_keys": unexpected_keys
        }
        
    except Exception as e:
        LOGGER.error(f"æƒé‡åŠ è½½å¤±è´¥: {e}")
        return {"status": "failed", "reason": str(e)}


def load_yolov8_backbone_weights(model: nn.Module, 
                                weights_path: Union[str, Path]) -> Dict:
    """
    ä»…åŠ è½½YOLOv8éª¨å¹²ç½‘ç»œæƒé‡
    
    Args:
        model: ç›®æ ‡æ¨¡å‹
        weights_path: æƒé‡æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict: åŠ è½½ç»“æœ
    """
    # åªåŠ è½½backboneç›¸å…³çš„æƒé‡
    exclude_keys = ['head', 'detect', 'anchor', 'loss']
    
    return load_pretrained_weights(
        model=model,
        weights_path=weights_path,
        strict=False,
        exclude_keys=exclude_keys
    )


def initialize_domain_adaptation_weights(model: nn.Module, 
                                       feature_dim: int = 256) -> None:
    """
    åˆå§‹åŒ–åŸŸé€‚åº”ç›¸å…³çš„æƒé‡
    
    Args:
        model: æ¨¡å‹
        feature_dim: ç‰¹å¾ç»´åº¦
    """
    # åˆå§‹åŒ–åŸŸé€‚åº”ç›¸å…³å±‚çš„æƒé‡
    for name, module in model.named_modules():
        if 'domain' in name.lower() or 'discriminator' in name.lower():
            if isinstance(module, nn.Linear):
                # Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
                    
            elif isinstance(module, nn.Conv2d):
                # Heåˆå§‹åŒ–
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
                    
            elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
                
    LOGGER.info("åŸŸé€‚åº”æƒé‡åˆå§‹åŒ–å®Œæˆ")


def save_mfdam_checkpoint(model: nn.Module,
                         optimizer: torch.optim.Optimizer,
                         epoch: int,
                         save_path: Union[str, Path],
                         **kwargs) -> None:
    """
    ä¿å­˜MFDAMæ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰è½®æ•°
        save_path: ä¿å­˜è·¯å¾„
        **kwargs: å…¶ä»–è¦ä¿å­˜çš„ä¿¡æ¯
    """
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        **kwargs
    }
    
    torch.save(checkpoint, save_path)
    LOGGER.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")


def load_mfdam_checkpoint(checkpoint_path: Union[str, Path],
                         model: Optional[nn.Module] = None,
                         optimizer: Optional[torch.optim.Optimizer] = None,
                         device: str = 'cpu') -> Dict:
    """
    åŠ è½½MFDAMæ£€æŸ¥ç‚¹
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        model: æ¨¡å‹ (å¯é€‰)
        optimizer: ä¼˜åŒ–å™¨ (å¯é€‰)
        device: è®¾å¤‡
        
    Returns:
        Dict: æ£€æŸ¥ç‚¹ä¿¡æ¯
    """
    checkpoint_path = Path(checkpoint_path)
    
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    if model is not None and 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
        LOGGER.info("æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
        
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer is not None and 'optimizer' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer'])
        LOGGER.info("ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å®Œæˆ")
        
    return checkpoint


class WeightManager:
    """
    æƒé‡ç®¡ç†å™¨
    
    ç”¨äºç®¡ç†MFDAMè®­ç»ƒè¿‡ç¨‹ä¸­çš„æƒé‡åŠ è½½ã€ä¿å­˜å’Œæ›´æ–°
    """
    
    def __init__(self, save_dir: Union[str, Path]):
        """
        åˆå§‹åŒ–æƒé‡ç®¡ç†å™¨
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
    def save_best_weights(self, 
                         model: nn.Module,
                         metric_value: float,
                         metric_name: str = 'mAP') -> None:
        """
        ä¿å­˜æœ€ä½³æƒé‡
        
        Args:
            model: æ¨¡å‹
            metric_value: æŒ‡æ ‡å€¼
            metric_name: æŒ‡æ ‡åç§°
        """
        save_path = self.save_dir / f'best_{metric_name}.pt'
        
        checkpoint = {
            'model': model.state_dict(),
            f'best_{metric_name}': metric_value,
            'timestamp': torch.tensor(time.time())
        }
        
        torch.save(checkpoint, save_path)
        LOGGER.info(f"æœ€ä½³{metric_name}æƒé‡å·²ä¿å­˜: {metric_value:.4f}")
        
    def save_epoch_weights(self,
                          model: nn.Module,
                          epoch: int,
                          **kwargs) -> None:
        """
        ä¿å­˜epochæƒé‡
        
        Args:
            model: æ¨¡å‹
            epoch: è½®æ•°
            **kwargs: å…¶ä»–ä¿¡æ¯
        """
        save_path = self.save_dir / f'epoch_{epoch}.pt'
        
        checkpoint = {
            'epoch': epoch,
            'model': model.state_dict(),
            **kwargs
        }
        
        torch.save(checkpoint, save_path)
        
    def load_latest_weights(self, model: nn.Module) -> Optional[int]:
        """
        åŠ è½½æœ€æ–°çš„æƒé‡
        
        Args:
            model: æ¨¡å‹
            
        Returns:
            Optional[int]: æœ€åçš„epochæ•°ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # æŸ¥æ‰¾æ‰€æœ‰epochæƒé‡æ–‡ä»¶
        epoch_files = list(self.save_dir.glob('epoch_*.pt'))
        
        if not epoch_files:
            LOGGER.info("æœªæ‰¾åˆ°epochæƒé‡æ–‡ä»¶")
            return None
            
        # æ‰¾åˆ°æœ€å¤§çš„epochæ•°
        latest_epoch = max([
            int(f.stem.split('_')[1]) for f in epoch_files
        ])
        
        latest_file = self.save_dir / f'epoch_{latest_epoch}.pt'
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(latest_file, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
        
        LOGGER.info(f"å·²åŠ è½½æœ€æ–°æƒé‡: epoch {latest_epoch}")
        return latest_epoch