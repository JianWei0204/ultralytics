# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import os
import yaml
from torch.utils.data import DataLoader

from ultralytics.utils import LOGGER, RANK
from ultralytics.utils.torch_utils import de_parallel
from ultralytics.models.yolo.detect import DetectionTrainer

from trans_discriminator import TransformerDiscriminator
from feature_extractor import FeatureExtractor


class DomainAdaptTrainer(DetectionTrainer):
    """
    Domain Adaptation Trainer for YOLOv8 object detection.
    """

    def __init__(self, cfg=None, overrides=None, _callbacks=None):
        # ä»…ä½¿ç”¨æ ‡å‡†YOLOv8å‚æ•°åˆå§‹åŒ–
        super().__init__(cfg, overrides, _callbacks)

        # åˆå§‹åŒ–åŸŸé€‚åº”ç»„ä»¶å’Œå‚æ•°
        self.target_data = None
        self.target_dataset = None
        self.target_loader = None
        self.disc_lr = 0.001  # é»˜è®¤åˆ¤åˆ«å™¨å­¦ä¹ ç‡

        # æºåŸŸå’Œç›®æ ‡åŸŸæ ‡ç­¾
        self.source_label = 0
        self.target_label = 1

        # åŸŸé€‚åº”ç»„ä»¶
        self.discriminator = None
        self.optimizer_D = None
        self.feature_extractor = None
        self.domain_adapt_enabled = False

    def setup_domain_adaptation(self, target_data, disc_lr=0.001):
        """è®¾ç½®åŸŸé€‚åº”è®­ç»ƒéœ€è¦çš„å‚æ•°å’Œç»„ä»¶"""
        self.target_data = target_data
        self.disc_lr = disc_lr
        self.domain_adapt_enabled = True

        LOGGER.info(f"Domain adaptation enabled with target data: {self.target_data}")
        LOGGER.info(f"Discriminator learning rate set to: {self.disc_lr}")

    def _setup_train(self, world_size):
        """è®¾ç½®è®­ç»ƒä¸åŸŸé€‚åº”ç»„ä»¶"""
        # åˆå§‹åŒ–æ ‡å‡†è®­ç»ƒè®¾ç½®
        super()._setup_train(world_size)

        # å¦‚æœå¯ç”¨äº†åŸŸé€‚åº”ï¼Œåˆ™è®¾ç½®ç›¸å…³ç»„ä»¶
        if self.domain_adapt_enabled:
            # åˆ›å»ºç‰¹å¾æå–å™¨è®¿é—®æ¡¥æ¥å±‚è¾“å‡º
            self.feature_extractor = FeatureExtractor(self.model, 'Adjust_Transformer')

            # å°†æºåŸŸè®­ç»ƒæ•°æ®åŠ è½½å™¨ä¿å­˜ä¸ºsource_loader
            self.source_loader = self.train_loader

            # åŠ è½½ç›®æ ‡åŸŸæ•°æ®é›†å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨
            LOGGER.info(f"Loading target domain dataset: {self.target_data}")

            # è§£æç›®æ ‡åŸŸYAMLæ–‡ä»¶ä»¥è·å–å®é™…å›¾åƒè·¯å¾„
            try:
                with open(self.target_data, 'r') as f:
                    target_yaml = yaml.safe_load(f)

                # è·å–åŸºç¡€è·¯å¾„å’Œè®­ç»ƒå›¾åƒè·¯å¾„
                base_path = target_yaml.get('path', '')
                train_path = target_yaml.get('train', '')

                if not base_path or not train_path:
                    raise ValueError(f"Missing 'path' or 'train' in {self.target_data}")

                # æ„å»ºå®Œæ•´çš„è®­ç»ƒå›¾åƒè·¯å¾„
                target_img_path = os.path.join(base_path, train_path)
                LOGGER.info(f"Target domain train path: {target_img_path}")

                # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                if not os.path.exists(target_img_path):
                    raise FileNotFoundError(f"Target domain path does not exist: {target_img_path}")

                # åˆ›å»ºç›®æ ‡åŸŸæ•°æ®é›†
                self.target_dataset = self.build_dataset(
                    img_path=target_img_path,
                    mode="train",
                    batch=self.args.batch
                )

                # ç›´æ¥åˆ›å»ºDataLoader
                batch_size = self.batch_size // max(world_size, 1)
                nw = min([os.cpu_count() // max(world_size, 1), self.args.workers, 8])
                collate_fn = self.target_dataset.collate_fn

                if RANK == -1:
                    # éåˆ†å¸ƒå¼è®­ç»ƒ
                    self.target_loader = DataLoader(
                        dataset=self.target_dataset,
                        batch_size=batch_size,
                        shuffle=True,
                        num_workers=nw,
                        collate_fn=collate_fn,
                        pin_memory=True,
                        drop_last=True,  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
                    )
                else:
                    # åˆ†å¸ƒå¼è®­ç»ƒ
                    sampler = torch.utils.data.distributed.DistributedSampler(
                        self.target_dataset, shuffle=True
                    )
                    self.target_loader = DataLoader(
                        dataset=self.target_dataset,
                        batch_size=batch_size,
                        sampler=sampler,
                        num_workers=nw,
                        collate_fn=collate_fn,
                        pin_memory=True,
                        drop_last=True,
                    )

                # åˆå§‹åŒ–åˆ¤åˆ«å™¨åŠå…¶ä¼˜åŒ–å™¨
                self.setup_discriminator()

                LOGGER.info(f"Target domain dataloader created with {len(self.target_loader)} batches")

            except Exception as e:
                LOGGER.error(f"Error loading target domain dataset: {e}")
                raise

    def setup_discriminator(self):
        """åˆå§‹åŒ–åŸŸåˆ¤åˆ«å™¨åŠå…¶ä¼˜åŒ–å™¨"""
        # æ¡¥æ¥å±‚ç‰¹å¾é€šé“æ•°
        feature_channels = 128  # ä»yamlæ–‡ä»¶ä¸­çš„Adjust_Transformerå±‚è·å–

        # åˆ›å»ºåˆ¤åˆ«å™¨
        self.discriminator = TransformerDiscriminator(channels=feature_channels)
        self.discriminator.train()
        self.discriminator.cuda()

        if RANK != -1:
            self.discriminator = nn.parallel.DistributedDataParallel(
                self.discriminator, device_ids=[RANK]
            )

        # åˆ›å»ºåˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
        self.optimizer_D = torch.optim.Adam(
            self.discriminator.parameters(),
            lr=self.disc_lr,
            betas=(0.9, 0.99)
        )
        self.optimizer_D.zero_grad()

        LOGGER.info(f"Discriminator initialized with learning rate {self.disc_lr}")

    def weightedMSE(self, D_out, label):
        """è®¡ç®—åˆ¤åˆ«å™¨çš„åŠ æƒMSEæŸå¤±"""
        return torch.mean((D_out - label.cuda()).abs() ** 2)

    def adjust_learning_rate_D(self, epoch):
        """åˆ¤åˆ«å™¨çš„å¤šé¡¹å¼å­¦ä¹ ç‡è¡°å‡"""

        def lr_poly(base_lr, iter, max_iter, power):
            return base_lr * ((1 - float(iter) / max_iter) ** power)

        lr = lr_poly(self.disc_lr, epoch, self.epochs, 0.8)
        for param_group in self.optimizer_D.param_groups:
            param_group['lr'] = lr
        return lr

    def _do_train(self, world_size=1):
        """ä¿®æ”¹åçš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…å«åŸŸé€‚åº”"""
        # å¦‚æœåŸŸé€‚åº”æœªå¯ç”¨ï¼Œåˆ™æ‰§è¡Œæ ‡å‡†è®­ç»ƒ
        if not self.domain_adapt_enabled:
            LOGGER.info("Domain adaptation not enabled, performing standard training")
            return super()._do_train(world_size)

        # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒå’Œä¸€èˆ¬è®­ç»ƒç»„ä»¶
        if world_size > 1:
            self._setup_ddp(world_size)
        self._setup_train(world_size)

        self.epoch_time = None
        self.epoch_time_start = time.time()
        self.train_time_start = time.time()
        self.run_callbacks("on_train_start")

        LOGGER.info(f'Image sizes {self.args.imgsz} train, {self.args.imgsz} val\n'
                    f'Using {self.source_loader.num_workers * (world_size or 1)} dataloader workers\n'
                    f"Logging results to {self.save_dir}\n"
                    f'Starting domain adaptation training for {self.epochs} epochs...')

        for epoch in range(self.start_epoch, self.epochs):
            self.epoch = epoch
            self.run_callbacks("on_train_epoch_start")
            self.model.train()

            # è°ƒæ•´åˆ¤åˆ«å™¨å­¦ä¹ ç‡
            cur_lr_d = self.adjust_learning_rate_D(epoch)

            if RANK != -1:
                self.source_loader.sampler.set_epoch(epoch)
                self.target_loader.sampler.set_epoch(epoch)

            # åˆ›å»ºæºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®è¿­ä»£å™¨
            target_iter = iter(self.target_loader)
            source_iter = iter(self.source_loader)

            nb = len(self.source_loader)
            self.tloss = None

            # è¿›åº¦æ¡
            if RANK in (-1, 0):
                LOGGER.info(self.progress_string())
                pbar = self.progress_bar(nb)
            else:
                pbar = range(nb)

            self.optimizer.zero_grad()
            self.optimizer_D.zero_grad()

            for i in pbar:
                self.run_callbacks("on_train_batch_start")

                # å¤„ç†ç›®æ ‡åŸŸæ•°æ®ï¼ˆç”¨äºåŸŸé€‚åº”ï¼‰
                try:
                    target_batch = next(target_iter)
                except StopIteration:
                    target_iter = iter(self.target_loader)
                    target_batch = next(target_iter)

                target_batch = self.preprocess_batch(target_batch)

                # 1. è®­ç»ƒç”Ÿæˆå™¨ - å†»ç»“åˆ¤åˆ«å™¨å‚æ•°
                for param in self.discriminator.parameters():
                    param.requires_grad = False

                # å°†ç›®æ ‡åŸŸæ•°æ®é€šè¿‡æ¨¡å‹å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast(self.amp):
                    _ = self.model(target_batch["img"])
                    # ä½¿ç”¨ç‰¹å¾æå–å™¨è·å–æ¡¥æ¥å±‚ç‰¹å¾
                    target_features = self.feature_extractor.get_features()

                # ä¸Šé‡‡æ ·ç‰¹å¾åˆ°é€‚åˆåˆ¤åˆ«å™¨çš„å°ºå¯¸
                interp = nn.Upsample(size=(128, 128), mode='bilinear', align_corners=True)
                target_features_up = interp(target_features)

                # å°†ç‰¹å¾é€šè¿‡åˆ¤åˆ«å™¨ï¼Œæ ‡è®°ä¸ºæºåŸŸï¼ˆå¯¹æŠ—æ€§ï¼‰
                D_out = self.discriminator(F.softmax(target_features_up, dim=1))
                D_source_label = torch.FloatTensor(D_out.data.size()).fill_(self.source_label)

                # è®¡ç®—å¯¹æŠ—æŸå¤±
                loss_adv = 0.1 * self.weightedMSE(D_out, D_source_label)

                if self.is_valid_number(loss_adv.data.item()):
                    self.scaler.scale(loss_adv).backward()

                # å¤„ç†æºåŸŸæ•°æ®ï¼ˆç”¨äºæ ‡å‡†è®­ç»ƒï¼‰
                try:
                    source_batch = next(source_iter)
                except StopIteration:
                    source_iter = iter(self.source_loader)
                    source_batch = next(source_iter)

                source_batch = self.preprocess_batch(source_batch)

                # ä½¿ç”¨æºåŸŸæ•°æ®è¿›è¡Œæ ‡å‡†å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast(self.amp):
                    loss = self.model(source_batch["img"])
                    # è·å–æ¡¥æ¥å±‚ç‰¹å¾
                    source_features = self.feature_extractor.get_features()

                if self.is_valid_number(loss.data.item()):
                    self.scaler.scale(loss).backward()

                # 2. è®­ç»ƒåˆ¤åˆ«å™¨ - è§£å†»åˆ¤åˆ«å™¨å‚æ•°
                for param in self.discriminator.parameters():
                    param.requires_grad = True

                # åˆ†ç¦»ç‰¹å¾ä»¥é¿å…åå‘ä¼ æ’­åˆ°ç”Ÿæˆå™¨
                target_features = target_features.detach()
                source_features = source_features.detach()

                # ä¸Šé‡‡æ ·ç‰¹å¾
                target_features_up = interp(target_features)
                source_features_up = interp(source_features)

                # ç›®æ ‡åŸŸåˆ†ç±»
                D_out_t = self.discriminator(F.softmax(target_features_up, dim=1))
                D_target_label = torch.FloatTensor(D_out_t.data.size()).fill_(self.target_label)
                loss_d_t = 0.1 * self.weightedMSE(D_out_t, D_target_label)

                if self.is_valid_number(loss_d_t.data.item()):
                    loss_d_t.backward()

                # æºåŸŸåˆ†ç±»
                D_out_s = self.discriminator(F.softmax(source_features_up, dim=1))
                D_source_label = torch.FloatTensor(D_out_s.data.size()).fill_(self.source_label)
                loss_d_s = 0.1 * self.weightedMSE(D_out_s, D_source_label)

                if self.is_valid_number(loss_d_s.data.item()):
                    loss_d_s.backward()

                # æ›´æ–°å‚æ•°
                self.optimizer_step()
                self.optimizer_D.step()
                self.optimizer_D.zero_grad()

                # æ›´æ–°è¿›åº¦æ¡
                if RANK in (-1, 0):
                    # åœ¨è¿›åº¦æ˜¾ç¤ºä¸­åŒ…å«åŸŸé€‚åº”æŸå¤±
                    mem = f"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G"
                    adv_loss = loss_adv.item() if 'loss_adv' in locals() else 0
                    d_loss = (loss_d_t.item() + loss_d_s.item()) / 2 if 'loss_d_t' in locals() else 0

                    pbar.set_description(
                        f"{epoch + 1}/{self.epochs} {mem} {loss.item():.4g} {adv_loss:.4g} {d_loss:.4g}"
                    )

                self.run_callbacks("on_train_batch_end")

            # è®­ç»ƒå¾ªç¯ç»“æŸéƒ¨åˆ†
            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}
            self.lr["lr/discriminator"] = cur_lr_d

            self.run_callbacks("on_train_epoch_end")

            # éªŒè¯å’Œæ¨¡å‹ä¿å­˜
            if RANK in (-1, 0):
                self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])

                # éªŒè¯
                if self.args.val or epoch + 1 == self.epochs:
                    self.metrics, self.fitness = self.validate()

                # ä¿å­˜æ¨¡å‹
                if (epoch + 1) % self.args.save_period == 0 or epoch + 1 == self.epochs:
                    self.save_model()

                    # ä¿å­˜åˆ¤åˆ«å™¨
                    discriminator_state = {
                        'epoch': epoch,
                        'state_dict': de_parallel(self.discriminator).state_dict(),
                        'optimizer': self.optimizer_D.state_dict(),
                        'disc_lr': self.disc_lr
                    }

                    torch.save(
                        discriminator_state,
                        os.path.join(self.save_dir, f'd_checkpoint_e{epoch + 1}.pt')
                    )

            # æ›´æ–°è°ƒåº¦å™¨
            self.scheduler.step()

            torch.cuda.empty_cache()

            # æ£€æŸ¥æ˜¯å¦æ—©åœ
            if self.stopper(epoch + 1, self.fitness):
                break

        # è®­ç»ƒç»“æŸ
        if RANK in (-1, 0):
            LOGGER.info(f"\n{self.epochs} epochs completed in "
                        f"{(time.time() - self.train_time_start) / 3600:.3f} hours.")
            self.final_eval()
            if self.args.plots:
                self.plot_metrics()
            self.run_callbacks("on_train_end")

        torch.cuda.empty_cache()
        self.run_callbacks("teardown")

    def is_valid_number(self, x):
        """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—"""
        import math
        return not (math.isnan(x) or math.isinf(x) or x > 1e4)

    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œåˆ¤åˆ«å™¨"""
        # é¦–å…ˆä¿å­˜æ ‡å‡†æ¨¡å‹
        super().save_model()

        # å¦‚æœåŸŸé€‚åº”å¯ç”¨ï¼Œä¿å­˜åˆ¤åˆ«å™¨
        if self.domain_adapt_enabled and hasattr(self, 'discriminator') and self.discriminator is not None:
            discriminator_state = {
                'epoch': self.epoch,
                'state_dict': de_parallel(self.discriminator).state_dict(),
                'optimizer': self.optimizer_D.state_dict(),
                'disc_lr': self.disc_lr
            }

            # ä¿å­˜æœ€æ–°çŠ¶æ€
            disc_last_path = os.path.join(self.wdir, 'discriminator_last.pt')
            torch.save(discriminator_state, disc_last_path)

            # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜å¯¹åº”çš„åˆ¤åˆ«å™¨
            if self.best_fitness == self.fitness:
                disc_best_path = os.path.join(self.wdir, 'discriminator_best.pt')
                torch.save(discriminator_state, disc_best_path)